<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<title>Domov</title>
	<subtitle>Monikin ITinerár</subtitle>
	
	<link href="https://monikajassova.github.io/feed/feed.xml" rel="self"/>
	<link href="https://monikajassova.github.io"/>
	<updated>2022-06-07T00:00:00-00:00</updated>
	<id>https://monikajassova.github.io</id>
	<author>
		<name>Monika Jaššová</name>
		<email></email>
	</author>
	
	<entry>
		<title>Moja cesta do sveta IT</title>
		<link href="https://monikajassova.github.io/posts/moja-cesta/"/>
		<updated>2022-02-02T00:00:00-00:00</updated>
		<id>https://monikajassova.github.io/posts/moja-cesta/</id>
		<content type="html">
		  &lt;p&gt;&lt;em&gt;V prvom príspevku svojho blogu začínam zľahka a osobnejšie&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Pracujem ako softvérový tester a tento vzrušujúci a pestrý svet milujem. Do IT som sa však dostala okľukou. Keď som si na strednej v Turbo Pascale naprogramovala &amp;quot;Matrix&amp;quot; (rozumej vypisovanie náhodných reťazcov v zelenej farbe na čiernom pozadí), cítila som sa ako majster sveta 😄. Pre zaujímavosť, zhruba v tom čase vznikol YouTube a prvé videá sme pozerali práve na hodine informatiky. Keď prišlo na rozhodovanie čo po gymnáziu, zvažovala som aj informatiku, no pred elektrotechnikou a fyzikou som mala priveľký rešpekt. V mojom okolí sa nenašiel nikto, kto by tento smer schvaľoval a navrhol aspoň to skúsiť. Vybrala som si teda bežnejšiu a istejšiu cestu a išla na ekonómiu.&lt;/p&gt;
&lt;p&gt;Po vysokej škole som sa zamestnala ako účtovníčka a keď sa vrátila pani, ktorú som zastupovala počas materskej dovolenky, musela som sa porúčať. Dlho sa mi nedarilo nájsť nové zamestnanie, dokonca ani magentové Téčko, kam vraj berú každého, kto ovláda angličtinu, ma nikdy nepozvalo na pohovor. Až mi raz Facebook ukázal príspevok spolužiaka, že IT firma, v ktorej pracuje, otvára v spolupráci s SOŠ trojročné nadstavbové štúdium s garantovaným pracovným miestom pre absolventov. Tak som si povedala, že kedy, keď nie teraz a prihlásila sa.&lt;/p&gt;
&lt;p&gt;Zasadla som opäť do školských lavíc, v 28 rokoch ako najstaršia v ročníku a ako jediná žena. Čakal ma rok a pol denného štúdia a potom prax vo firme (po 3mes. skúšobnej dobe zmluva na TPP), všetko som zvládla bez problému a úspešne štúdium dokončila.&lt;br&gt;
Vo firme som bola pridelená na projekty, kde sa manuálne testovali webstránky, mobilné a desktopové aplikácie. Ja som však chcela kódiť a keď tá možnosť neprichádzala, zmenila som zamestnávateľa.&lt;/p&gt;
&lt;p&gt;V novej firme som sa dostala do čerstvo sformovaného big data tímu, kde som mala zastrešiť automatizovaný testing. Bola som sám tester v tíme, o doméne big data som nevedela nič a programovací jazyk Scala bol pre mňa tiež nový. Kolegovia developeri boli na tom podobne a nikto iný vo firme sa big data nevenoval, takže nebolo s kým konzultovať. Začiatky teda boli ťažké, ale chytila som sa, big data sa mi zapáčili a veľa som sa naučila. Posledný rok môjho pôsobenia vo firme som dostávala developerské úlohy, keďže som si chcela skúsiť aj rolu developera.&lt;/p&gt;
&lt;p&gt;Počas korony prišla ponuka z domény blockchainu, ktorá ma osobne zaujíma a rozhodla som sa ju prijať. Znamenala hneď niekoľko zmien naraz: prácu na kontrakt, pre zahraničnú firmu s vlastným produktom, 100% remote a vo sfére open source. Verím, že sa tu opäť veľa naučím a posuniem odborne 🙂.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://monikajassova.github.io/img/journey.jpg&quot; alt=&quot;Cesta&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;moje-postrehy-a-odpor%C3%BA%C4%8Dania&quot;&gt;Moje postrehy a odporúčania &lt;a class=&quot;direct-link&quot; href=&quot;#moje-postrehy-a-odpor%C3%BA%C4%8Dania&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Nasledujú subjektívne názory vychádzajúce z osobných skúseností a hlavne z pohľadu testovania a vývoja&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;výhodou IT je, že šancu uplatniť sa máte aj v neskoršom veku. Stačí mať predpoklady (logické myslenie), chuť veľa sa učiť a vedieť po anglicky (aspoň tak, aby ste rozumeli študijným materiálom a dokumentácii). Hodí sa aj záujem o to, ako veci fungujú.&lt;/li&gt;
&lt;li&gt;ak ovládate angličtinu na komunikatívnej úrovni, otvárajú sa Vám formou práce z domu možnosti prakticky po celom svete. IT firmy majú nedostatok ľudí, často stačí, ak máte motiváciu, chýbajúce znalosti získate za pochodu. Aj keď samozrejme záleží na firme, 2 moje pohovory skončili hodnotením, že viac chcem ako viem a raz to znamenalo stopku, druhý raz postup do ďalšieho kola.&lt;/li&gt;
&lt;li&gt;IT je špecifické tým, že na prácu v ňom nepotrebujete formálne vzdelanie a študijné materiály, tutoriály nájdete voľne dostupné na internete. Existujú tiež online kurzy, prezenčná výučba, firmy majú tzv. farmy/akadémie, kde si vychovávajú ľudí, prípadne si môžete nájsť mentora. Záleží len na štýle učenia, ktorý Vám najviac vyhovuje a koľko času ste ochotní štúdiu venovať. Čítala som dokonca o panej, čo za 2 dni prešla kurz na automatizovaný testing v Seleniu, na štvrtý deň išla na pohovor a vzali ju.&lt;/li&gt;
&lt;li&gt;nemusíte hneď ovládať zložité koncepty, zložitejšie veci sa naučíte vtedy, keď ich budete potrebovať. Väčšina projektov nie je raketová veda, ale ničím výnimočné webové alebo mobilné aplikácie. V programovaní sa dá cvičením zlepšiť a rozvíjať potrebné myslenie (learning by doing - učenie sa robením). IT je rozsiahle, ľudia sa špecializujú (je lepšie poznať vybranú oblasť do hĺbky ako vedieť z každého rožku trošku). Ale je super, ak máte poznatky aj z príbuzných oblastí a všeobecný IT prehľad.&lt;/li&gt;
&lt;li&gt;je fajn, ak máte skúsenejších kolegov/mentorov, ktorí Vám poradia a potiahnu Vás. Podľa môjho názoru sa sami viete dostať len po určitú úroveň a metódou pokus-omyl sa síce poučíte, ale stojí to veľa času a úsilia.&lt;/li&gt;
&lt;li&gt;práca v IT (technická) je vlastne nikdy nekončiace sa štúdium. Neustále prichádzajú nové požiadavky, nové technológie, existujúce technológie sa vyvíjajú a zastarávajú.&lt;/li&gt;
&lt;li&gt;v IT je celá škála profesií, nie všetky sú technické. Okrem developera, testera, DevOps, architekta, databázového administrátora, systémového administrátora sa môžete uplatniť na manažérskych pozíciách (produktový, projektový manažér atď.), ako scrum master, product owner, technical writer (dokumentarista), biznis analytik, UI/UX dizajnér...&lt;/li&gt;
&lt;li&gt;je relatívne ľahké zmeniť svoje smerovanie, viete sa v rámci kariérnych ciest posúvať vertikálne aj horizontálne a prechádzať medzi vyššie spomenutými pozíciami&lt;/li&gt;
&lt;li&gt;práca je rôznorodá, projekt sa týka nejakej domény reálneho alebo virtuálneho sveta. Napr. pri SW na vyhotovovanie reportov z glukomeru potrebujete vedieť aj niečo o cukrovke, pri dátach z platformy pre online reklamy sa dozviete čo-to o digitálnom marketingu.&lt;/li&gt;
&lt;li&gt;vývoj SW nie je len o osamelom ťukaní do počítača, zahŕňa aj plánovanie práce, diskusie s kolegami týkajúce sa riešenia úloh, s ostatnými spolupracujúcimi profesiami, so zákazníkom a pod., takže je dôležitá komunikácia&lt;/li&gt;
&lt;li&gt;nie je sranda celé dni sedieť za počítačom (z fyzického ani psychického hľadiska), oplatí sa dbať na ergonómiu, psychohygienu a kompenzovať sedenie pohybom a cvičením, zdravie máme len jedno&lt;/li&gt;
&lt;li&gt;na záver na odľahčenie: nie všetci v IT sú kocky v nemoderných outfitoch a ponožkami v sandáloch 😄. Nájde sa pár takých, ináč sú tu normálni ľudia ako všade inde, introverti aj extroverti, veriaci aj neveriaci, metalisti, hiphoperi, športovci, umelecky založení, manuálne zruční... A že sa ajťáci nevedia baviť so ženami alebo nemajú partnerky je tiež len mýtus.&lt;/li&gt;
&lt;/ul&gt;

			
		</content>
	</entry>
	
	<entry>
		<title>Užitočné zdroje</title>
		<link href="https://monikajassova.github.io/posts/uzitocne-zdroje/"/>
		<updated>2022-04-04T00:00:00-00:00</updated>
		<id>https://monikajassova.github.io/posts/uzitocne-zdroje/</id>
		<content type="html">
		  &lt;p&gt;&lt;em&gt;Na tomto mieste nájdete zoznam rôznych užitočných odkazov, ktorý budem aktualizovať&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.freecodecamp.org/&quot;&gt;https://www.freecodecamp.org&lt;/a&gt; Bezplatné tutoriály rôzneho druhu vo forme článkov a videí, dajú sa nájsť aj rozsiahle do hĺbky spracované témy&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://testautomationu.applitools.com/&quot;&gt;https://testautomationu.applitools.com&lt;/a&gt; Bezplatné kurzy zamerané na automatizovaný testing, aj okrajové témy&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.baeldung.com/&quot;&gt;https://www.baeldung.com&lt;/a&gt; Jednohubky aj dlhšie návody hlavne z JVM sveta: Java, Spring, Scala, Kotlin a tiež Linux a počítačová veda&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://skillmea.sk/&quot;&gt;https://skillmea.sk&lt;/a&gt; Bývalý learn2code - taký zlatý štandard kurzov v slovenskom jazyku - od programovania cez testing a grafiku až po marketing a iné&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://acloudguru.com/&quot;&gt;https://acloudguru.com&lt;/a&gt; Kurzy zamerané hlavne na cloudové technológie (AWS, GCP, Azure), Linux, DevOps a pár ďalších okruhov. Súčasťou sú tzv. laby v účte na študijné účely, kde si môžete prakticky vyskúšať úlohy&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://checkmarx.com/try-codebashing&quot;&gt;https://checkmarx.com/try-codebashing&lt;/a&gt; Školenie aplikačnej bezpečnosti, interaktívne lekcie pekne vysvetlia jednotlivé typy zraniteľností, ako prebiehajú ich zneužitia a útoky a ako im predísť&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.oreilly.com/&quot;&gt;https://www.oreilly.com&lt;/a&gt; Vydavateľstvo odbornej literatúry známe knihami s ohrozenými živočíšnymi druhmi na obálke, poskytuje tiež prístup k videám, interaktívnym kurzom a školeniam naživo&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://junior.guru/&quot;&gt;https://junior.guru&lt;/a&gt; Výborne spracovaný český web pre záujemcov o IT, v príručke rozoberá, čo si predstaviť pod programovaním, čo naňho potrebujete, ako sa učiť, ako zohnať prvú prácu aj ako sa posunúť od juniora ďalej&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://modernafirma.podbean.com/&quot;&gt;https://modernafirma.podbean.com&lt;/a&gt; Podcast Grishiho zo štúdia Wezeo a web inštruktora Yablka na témy v 3 oblastiach: premena na modernú firmu, budovanie digitálneho produktu, skúsenosti úspešných lídrov&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.glassdoor.com/&quot;&gt;https://www.glassdoor.com&lt;/a&gt; Anonymné recenzie firiem, informácie o platoch, hodnotenia pohovorov a otázky z nich&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.levels.fyi/&quot;&gt;https://www.levels.fyi&lt;/a&gt; Informácie o platoch v top firmách v IT odvetví&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cwjobs.co.uk/salary-checker/salary-calculator&quot;&gt;https://www.cwjobs.co.uk/salary-checker/salary-calculator&lt;/a&gt; Štatistiky o platoch v IT odvetví v UK&lt;/p&gt;
&lt;p&gt;Bonusový tip: veľa užitočného sa dozvedám prostredníctvom LinkedInu. Sledujte hashtagy, ktoré Vás zaujímajú, zapojte sa do skupín, nájdite si ľudí, ktorí tvoria dobrý obsah. Za mňa napr.:&lt;br&gt;
&lt;a href=&quot;https://sk.linkedin.com/in/zdenkovrabel&quot;&gt;Zdenko Vrabel&lt;/a&gt; - infrastructure engineer, píše o kóde, cloude a (samozrejme) infraštruktúre&lt;br&gt;
&lt;a href=&quot;https://sk.linkedin.com/in/jbednar&quot;&gt;Juraj Bednár&lt;/a&gt; - etický hacker, autor kurzov a kníh, venuje sa najmä slobode, kryptomenám, biohackingu a lepšiemu životu&lt;br&gt;
&lt;a href=&quot;https://sk.linkedin.com/in/filip-hric-11a5b1126&quot;&gt;Filip Hric&lt;/a&gt; - ambasádor Cypress (testovacieho frameworku pre weby) na Slovensku a autor kurzov&lt;/p&gt;

			
		</content>
	</entry>
	
	<entry>
		<title>Apache Spark</title>
		<link href="https://monikajassova.github.io/posts/spark/"/>
		<updated>2022-05-21T00:00:00-00:00</updated>
		<id>https://monikajassova.github.io/posts/spark/</id>
		<content type="html">
		  &lt;p&gt;&lt;em&gt;Zbežný prehľad fungovania Apache Spark&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt; je jedným z najpoužívanejších nástrojov pre analýzu dát vo veľkej škále (big data), rieši situáciu, keď sú dáta také veľké, že ich uloženie a spracovanie nedokáže jeden počítač. Zvláda dávkové (batch) spracovávanie dát, spracovávanie dát v reálnom čase (streaming), interaktívne dotazovanie, výpočty a operácie s grafom a strojové učenie (machine learning).&lt;br&gt;
Ide o open-source framework na spracovanie dát, ktorý dokáže rýchlo vykonávať procesovacie úlohy nad veľmi veľkými množinami dát a tiež dokáže tieto úlohy distribuovať medzi viacero počítačov, ktoré ich paralelne a nezávisle od seba spracúvajú. Pre svoje fungovanie vyžaduje manažér clustera počítačov (napr. Hadoop YARN - Yet Another Resource Negotiator) a distribuovaný systém súborov (napr. HDFS - Hadoop Distributed File System).&lt;br&gt;
Obrázok znázorňuje 4 knižnice pre rôzne účely a programovacie jazyky, ktoré podporuje základné API:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://monikajassova.github.io/img/SparkEcosystem.png&quot; alt=&quot;Spark Ecosystem&quot;&gt;&lt;/p&gt;
&lt;p&gt;Spark je napísaný v programovacom jazyku &lt;a href=&quot;https://www.scala-lang.org/&quot;&gt;Scala&lt;/a&gt;, prvá verzia vyšla v roku 2010 (od 2014 pod hlavičkou firmy Apache) a najnovšia verzia je 3.2.1 z januára 2022.&lt;br&gt;
Spočiatku sa Spark využíval na vlastnom &amp;quot;železe&amp;quot; - firmy vlastnili a starali sa o svoje Hadoop clustere, v dnešnej dobe cloudu sa už poskytuje ako manažovaná alebo serverless služba, menovite &lt;a href=&quot;https://aws.amazon.com/emr&quot;&gt;Amazon EMR&lt;/a&gt;, &lt;a href=&quot;https://azure.microsoft.com/en-us/services/databricks&quot;&gt;Azure Databricks&lt;/a&gt; alebo &lt;a href=&quot;https://cloud.google.com/solutions/spark&quot;&gt;Spark on Google Cloud&lt;/a&gt;, takže používateľom odpadá starosť o manažovanie clustera.&lt;/p&gt;
&lt;h3 id=&quot;spark-core&quot;&gt;Spark Core &lt;a class=&quot;direct-link&quot; href=&quot;#spark-core&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Je základným kameňom Sparku a ide o rozhranie pre programovanie aplikácií sústredené okolo abstrakcie nazývanej RDD.&lt;br&gt;
Najprv však priblížim 2 procesy, ktoré naštartuje Spark cluster po spustení Spark aplikácie:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;mark&gt;driver&lt;/mark&gt; je hlavný, riadiaci proces zodpovedný za vytvorenie Spark contextu, preklad kódu Spark aplikácie na logický plán DAG a následne na výpočtové jednotky, úlohy, ktoré sú distribuované medzi pracovné uzly (worker nodes). Taktiež koordinuje rozvrhnutie úloh a orchestráciu na každom exekútore.&lt;/li&gt;
&lt;li&gt;&lt;mark&gt;exekútory&lt;/mark&gt; bežia na pracovných uzloch clustra a sú zodpovedné za výkon im pridelených výpočtových úloh, vrátenie výsledkov driveru a tiež poskytnutie úložiska pre RDD. Medzivýsledky jednotlivých operácií sa držia v distribuovanej pamäti, neukladajú sa na disk (len ak sa nevojdú do RAM), preto sú veľmi rýchle.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Práve pre sprostredkovanie medzi týmito procesmi je potrebná nejaká forma cluster manažéra - okrem spomínaného YARNu sa novšie používa &lt;a href=&quot;https://spark.apache.org/docs/latest/running-on-kubernetes.html&quot;&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&quot;spark-rdd&quot;&gt;Spark RDD &lt;a class=&quot;direct-link&quot; href=&quot;#spark-rdd&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Resilient Distributed Dataset (RDD) je abstrakcia reprezentujúca nemennú kolekciu objektov, ktorá môže byť rozdistribuovaná medzi výpočtový cluster. Operácie nad RDD sa tiež dajú rozdeliť medzi uzly clustera a sú vykonávané v paralelnom dávkovom procese, čo vedie k rýchlemu a škálovateľnému procesovaniu.&lt;/p&gt;
&lt;p&gt;Spark RDD API uvádza niekoľko transformácií a akcií pre manipuláciu s RDD:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transformácie RDD vracajú vždy nový RDD a umožňujú vytvoriť závislosti medzi RDD. Každý RDD v reťazi závislostí má funkciu na výpočet svojich dát a smerník na svoj rodičovský RDD. Transformácia RDD je krok v programe hovoriaci Sparku ako získať dáta a čo s nimi robiť.&lt;/li&gt;
&lt;li&gt;Akcie nad RDD vracajú samotný výsledok, hodnotu (napr. reduce, count, collect). Výsledok môže byť uložený na disk, zapísaný do databázy alebo vypísaný do konzoly. RDD sú vyhodnocované odložene (lenivo - lazy evaluation), vyhodnotenie sa nezačne, kým nie je zavolaná nejaká akcia.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Resilient - odolný alebo schopný obnoviť činnosť - znamená, že ak v niektorom kroku procesu operácia/exekútor spadne, nie je to žiaden problém a potrebný RDD sa obnoví zo vstupných dát a danou reťazou závislostí a transformácií. Beh Spark aplikácie môže pokračovať, celkové spracovanie dát nie je ohrozené a je teda odolné voči zlyhaniam.&lt;/p&gt;
&lt;p&gt;Prvotný RDD sa vytvorí paralelným načítaním dát do zvláštnych partícií na jednotlivých pracovných uzloch. Takto má každý uzol inú podmnožinu dát - logickú časť veľkej distribuovanej množiny dát. Exekútory obdržia na spracovanie jednu alebo viac partícií. Exekútor vykonáva v danom čase vždy len jednu úlohu pre danú partíciu. Rozhodovanie o počte partícií je hľadanie kompromisu: ak rozdelíme RDD na veľa menších partícií, rozvrhovanie úloh môže zabrať viac času ako samotný výpočet a na druhej strane málo veľkých partícií môže znamenať, že nedôjde k vyťaženiu všetkých pracovných uzlov a nevyužijú sa tak výhody paralelizmu.&lt;/p&gt;
&lt;p&gt;RDD môžu byť vytvorené dvomi spôsobmi:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;odkazovaním sa na súbor dát v externom úložnom systéme, ako je zdieľaný súborový systém, HDFS, HBase, jednoduchý textový súbor, SQL databáza, NoSQL sklad (ako Cassandra a MongoDB), bucket na úložisku Amazon S3 a veľa iných,&lt;/li&gt;
&lt;li&gt;paralelizovaním existujúcej kolekcie v programe drivera - aplikovaním transformácií na existujúcich RDD. Okrem tradičnej map and reduce funkcionality má Spark tiež vstavanú podporu pre spájanie data setov, filtrovanie a agregácie.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&quot;dag&quot;&gt;DAG &lt;a class=&quot;direct-link&quot; href=&quot;#dag&quot;&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Druhou hlavnou abstrakciou v Sparku je DAG (directed acyclic graph). Spark definuje úlohy, ktoré môžu byť paralelne vypočítané nad partíciami dát v clusteri. Zostrojí logický tok operácií, ktorý je reprezentovaný ako riadený acyklický graf, kde uzly predstavujú RDD partície a hrany transformácie dát.&lt;br&gt;
Logický plán DAGu sa podľa povahy transformácií skonvertuje na &lt;mark&gt;fyzický exekučný plán&lt;/mark&gt; pozostávajúci z etáp (stages). Úlohy v každej etape sa zbalia spolu a pošlú exekútorom. Príklad takého plánu:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://monikajassova.github.io/img/plan.png&quot; alt=&quot;Príklad plánu&quot;&gt;&lt;/p&gt;
&lt;p&gt;Na etapu 1 nadväzuje etapa 2, etapa 3 je nezávislá na nich a môže bežať paralelne, v etape 4 sa spoja RDD z etáp 2 a 3.&lt;/p&gt;
&lt;h3 id=&quot;spark-sql&quot;&gt;Spark SQL &lt;a class=&quot;direct-link&quot; href=&quot;#spark-sql&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Spark SQL priniesol dátovú abstrakciu nazývanú DataFrame, táto poskytuje podporu pre štruktúrované a semištruktúrované dáta (JSON).&lt;br&gt;
DataFrame je distribuovaná kolekcia objektov typu Row organizovaná do pomenovaných stĺpcov, koncepčne zodpovedá tabuľke v relačnej databáze.&lt;br&gt;
DataFramy sú takisto ako RDD nemenné.&lt;br&gt;
Spark SQL poskytuje štandardné rozhranie pre čítanie dát z a zapisovanie do rôznych dátových úložísk vrátane JSON a CSV súborov, Apache Hive, JDBC, Apache Avro, Apache Parquet, existujú aj connectory pre MongoDB atď.&lt;br&gt;
Spark SQL umožňuje dotazovať dáta v rámci Spark aplikácií s použitím DataFrame API alebo SQL dotazov (slúži ako distribuovaný SQL query engine).&lt;/p&gt;
&lt;p&gt;Ukážka DataFrame API vs SQL:&lt;/p&gt;
&lt;pre class=&quot;language-scala&quot;&gt;&lt;code class=&quot;language-scala&quot;&gt;spark&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;read&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;table&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;published_mediabi.clicks&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;br&gt;  &lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;where&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;col&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;dt&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;2020-12-08&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;br&gt;  &lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;agg&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;sum&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;clicks&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;br&gt;&lt;br&gt;spark&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;sql&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;SELECT SUM(clicks) FROM published_mediabi.clicks WHERE dt = &#39;2020-12-08&#39;&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;spark-shell&quot;&gt;Spark Shell &lt;a class=&quot;direct-link&quot; href=&quot;#spark-shell&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Na rýchle ad-hoc analýzy, prototypovanie a testovanie sa používa Spark shell.&lt;br&gt;
Ak si chcete Spark prakticky vyskúšať na lokálnom počítači, je to možné práve v tejto konzole (stačí mať nainštalovanú Javu), po &lt;a href=&quot;https://spark.apache.org/downloads.html&quot;&gt;stiahnutí&lt;/a&gt; ju spustíte z rozbaleného adresára príkazom&lt;/p&gt;
&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;./bin/spark-shell&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;v rámci konzoly funguje automatické dokončovanie pomocou &lt;code&gt;TAB&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;pomoc zobrazíme &lt;code&gt;:help&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;históriu príkazov &lt;code&gt;:history&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;do paste módu pre vloženie viacriadkového kusu kódu sa dostaneme &lt;code&gt;:paste&lt;/code&gt; (a von z neho &lt;code&gt;CTRL&lt;/code&gt;+&lt;code&gt;D&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Scala skript môžeme načítať &lt;code&gt;:load &amp;lt;cesta k súboru&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;prácu v konzole ukončíme &lt;code&gt;:quit&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Spark shell pri spustení vytvorí:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;premennú &lt;mark&gt;sc&lt;/mark&gt; pre prístup k SparkContextu (vstupný bod Spark Core funkcionality)&lt;/li&gt;
&lt;li&gt;premennú &lt;mark&gt;spark&lt;/mark&gt; pre prístup k SparkSession (vstupný bod Spark SQL funkcionality)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Príklady načítania dát:&lt;/p&gt;
&lt;pre class=&quot;language-scala&quot;&gt;&lt;code class=&quot;language-scala&quot;&gt;spark&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;read&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;parquet&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;/data/staged/ott/mediabi/googledfp/NetworkImpressions/dt=20201209&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;br&gt;spark&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;read&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;option&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;header&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;csv&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;/data/staged/media/scv/bluekai_insights_categories/dt=20201209&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;br&gt;spark&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;read&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;table&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;staged_mediabi.dfp_etl_status&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Spustenie príkladov pribalených k inštalácii Sparku a celú dokumentáciu nájdete na &lt;a href=&quot;https://spark.apache.org/docs/latest/#running-the-examples-and-shell&quot;&gt;https://spark.apache.org/docs/latest/#running-the-examples-and-shell&lt;/a&gt;&lt;/p&gt;

			
		</content>
	</entry>
	
	<entry>
		<title>Testovanie ETL</title>
		<link href="https://monikajassova.github.io/posts/testovanie-etl/"/>
		<updated>2022-06-07T00:00:00-00:00</updated>
		<id>https://monikajassova.github.io/posts/testovanie-etl/</id>
		<content type="html">
		  &lt;p&gt;&lt;em&gt;Cieľom tohto článku je zosumarizovať postupy, ktoré som implementovala pri testovaní ETL procesov a s ktorými som sa stretla v rámci big data&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;%C3%BAvod-do-big-data-a-etl&quot;&gt;Úvod do big data a ETL &lt;a class=&quot;direct-link&quot; href=&quot;#%C3%BAvod-do-big-data-a-etl&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Veľké dáta (ďalej big data) sú charakterizované troma V:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;volume - veľký objem dát,&lt;/li&gt;
&lt;li&gt;velocity - rýchlosť s akou sú produkované (gigabajty nových dát denne),&lt;/li&gt;
&lt;li&gt;variety - dáta prichádzajú z rôznych zdrojov a v rôznej forme.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Sú to také veľké súbory dát, že ich nie je možné uchovávať, spravovať a spracovávať bežne používanými softvérovými prostriedkami v rozumnom čase.&lt;br&gt;
Úlohou dátových inžinierov je spracovanie big data a ich príprava do požadovanej podoby pre ďalších používateľov dát - spravidla dátových analytikov, dátových vedcov.&lt;/p&gt;
&lt;p&gt;Jeden z procesov, ktorým sa to deje, sa nazýva &lt;mark&gt;ETL&lt;/mark&gt; (extract-transform-load).&lt;br&gt;
V prvej fáze ETL sa dáta extrahujú zo zdrojov - sťahujú sa z FTP serverov, cloudových úložísk, dotazujú sa cez REST API a pod. Surové dáta môžu mať rôznu štruktúru - tabuľky v relačných databázach, záznamy v NoSQL databázach, súbory formátu CSV, XML, JSON atď.&lt;br&gt;
Počas fázy transformácie sa dáta upravujú podľa požadovaných pravidiel, filtrujú, očisťujú, spájajú s inými dátami, prevádzajú sa nad nimi agregácie alebo iné výpočty. Tieto medzivýsledky sa držia v oblasti staged.&lt;br&gt;
V záverečnej fáze sa finálne dáta publikujú na dohodnutom mieste, spravidla v dátovom sklade (data warehouse - DWH), kde ich priamo alebo cez connectory vedia používatelia dotazovať, pracovať s nimi, využiť ako podklad pre analytické a BI (business intelligence) nástroje.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://monikajassova.github.io/img/etl.png&quot; alt=&quot;ETL proces&quot;&gt;&lt;/p&gt;
&lt;p&gt;Novším prístupom je &lt;mark&gt;ELT&lt;/mark&gt; (extract-load-transform) proces, pri ktorom sa surové dáta nahrajú na cieľový systém a transformácie sa vykonávajú až tam, napr. s pomocou nástroja &lt;a href=&quot;https://www.getdbt.com/&quot;&gt;dbt&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;ETL procesy môžu prebiehať:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dávkovo - &lt;mark&gt;batch processing&lt;/mark&gt;, kedy sú spúšťané periodicky, riadené orchestrátorom&lt;/li&gt;
&lt;li&gt;ako &lt;mark&gt;streaming&lt;/mark&gt;, kedy sú dáta spracúvané v reálnom čase ako prichádzajú do systému&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Na našom projekte sme uplatňovali batch processing a ako orchestrátor pre definovanie workflowow a plánovanie ich behov sme používali &lt;a href=&quot;https://oozie.apache.org/&quot;&gt;Oozie&lt;/a&gt; a neskôr aj &lt;a href=&quot;https://airflow.apache.org/&quot;&gt;Airflow&lt;/a&gt;. Ukážka ako taký workflow môže vyzerať (&lt;a href=&quot;https://www.aakashpydi.com/jetstream-architecture/&quot;&gt;zdroj obrázka&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://monikajassova.github.io/img/airflow.png&quot; alt=&quot;Workflow v Airflow&quot;&gt;&lt;/p&gt;
&lt;p&gt;Na samotné spracúvanie dát sme používali Apache Spark, o ktorom som písala &lt;a href=&quot;https://monikajassova.github.io/posts/spark&quot;&gt;v tomto príspevku&lt;/a&gt;. Pre predstavu na akom HW Spark a Hadoop bežal, išlo o Oracle BDA/HDFS cluster o 12 uzloch (každý s procesorom Intel XEON), ktorý dokopy disponoval 1 TB RAM a 900 TB HDD.&lt;/p&gt;
&lt;p&gt;Testovanie a zabezpečenie kvality ETL procesov bolo náplňou mojej práce v DAZN. Keď som nastúpila, big data boli pre mňa nové a netušila som, ako to budem testovať. Online zdrojov k téme testovania dát je poskromne aj teraz a keď som začínala, nachádzala som len odkazy na platené nástroje, ale máločo o konkrétnych metódach. Názvy testov v nasledujúcom texte sú interné označenia, nejde o odbornú terminológiu.&lt;/p&gt;
&lt;h3 id=&quot;d%C3%A1tov%C3%A9-e2e-testy&quot;&gt;Dátové E2E testy &lt;a class=&quot;direct-link&quot; href=&quot;#d%C3%A1tov%C3%A9-e2e-testy&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Prvým krokom pri zavedení automatizovaného testovania jednotlivých ETL procesov bolo vytvorenie niečoho ako E2E testov nad fixnými dátami. V prostredí som vytvorila osobitné priečinky, kde som si pripravila súbory s testovacími dátami po vzore reálnych. Malú vzorku, tak aby som mala kontrolu nad tým, čo v nej je a čo sa s ňou deje. Pripravila som dáta pre rôzne testovacie prípady, tzv. happy path aj okrajové prípady, či sa do výsledku dostávajú skutočne tie dáta, ktoré majú.&lt;br&gt;
Nad týmito dátami som púšťala workflowy, ktoré ich spracovali a na konci sa výsledné dáta porovnali s očakávaným výsledkom. Automatizovaný test alebo pomocný skript na záver všetky výsledné dáta aj medzivýsledky ETL procesu zmazal a pripravil prostredie na ďalší beh celého testovacieho workflowu.&lt;br&gt;
Ak bol ETL proces jednoduchý a napr. zabezpečoval len, aby sa v published držala najnovšia verzia záznamu, automatizované testy bežali na produkčných dátach a porovnávali vstupné dáta s výstupnými.&lt;/p&gt;
&lt;p&gt;E2E testy pomohli vyriešiť záhadný bug - mala som podozrenie, že občas majú niektoré záznamy isté hodnoty dvojnásobné ako by mali byť, ale nevedeli sme kedy a prečo sa tak deje. Až keď som sa dostala k nasimulovaniu celého procesu a dát, tak som zistila, že v špecifickom prípade, keď dáta spadali do dvoch kategórií, tieto sa započítali dvakrát.&lt;/p&gt;
&lt;h3 id=&quot;testy-konzistentnosti&quot;&gt;Testy konzistentnosti &lt;a class=&quot;direct-link&quot; href=&quot;#testy-konzistentnosti&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Ďalšou skupinou automatizovaných testov boli testy, ktoré zisťovali, či sú produkčné dáta konzistentné z hľadiska logiky domény. Príklad z digitálneho marketingu: počet kliknutí na reklamu musí byť menší ako počet videní a ten zas menší ako počet požiadaviek o reklamu odoslaných serveru.&lt;/p&gt;
&lt;p&gt;Testy konzistentnosti sme vo veľkom využili pri implementácii produktu single customer view v grafovej databáze Neo4j. Grafová databáza je koncept umožňujúci jednoducho modelovať vzťahy reálne existujúce v doméne. Ukážka vzťahov medzi rôznymi uzlami a dopytovacieho jazyka Cypher:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://monikajassova.github.io/img/cypher.png&quot; alt=&quot;Neo4j&quot;&gt;&lt;/p&gt;
&lt;p&gt;Testy overovali, či skutočný stav zodpovedá navrhnutému dátovému modelu grafu - či sú správne vlastnosti uzlov a vzťahov, či nie sú vzťahy duplicitné, či sa v grafe nenachádzajú entity po TTL (time-to-live). Testy konzistentnosti nám pomohli nájsť chyby v implementácii grafu, identifikovať veci, ktoré sme doposiaľ pri implementácii neošetrili a tiež skryté zlyhania workflowov.&lt;/p&gt;
&lt;h3 id=&quot;monitoring&quot;&gt;Monitoring &lt;a class=&quot;direct-link&quot; href=&quot;#monitoring&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Tretím pilierom, ktorý nám veľmi pomohol odhaľovať problémy a anomálie a zvýšiť dôveru vo fungovanie našich ETL procesov, bolo nastavenie monitoringu metrík týkajúcich sa dát a samotných ETL procesov. Použili sme na to &lt;a href=&quot;https://www.elastic.co/kibana&quot;&gt;Kibanu&lt;/a&gt; od Elasticu (existuje veľa alternatív, napr. open-source &lt;a href=&quot;https://grafana.com/&quot;&gt;Grafana&lt;/a&gt;). Vytypovali sme metriky, ktoré sa opäť periodicky spúšťali ako Spark aplikácie (metriky týkajúce sa súborov na HDFS a tabuliek v Hive), Scala aplikácie (metriky týkajúce sa grafu v Neo4j a workflowov v Oozie) alebo bash skripty (metriky týkajúce sa tabuliek v BigQuery) a zaznamenávali hodnoty v logoch pre Kibanu.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://monikajassova.github.io/img/metric.png&quot; alt=&quot;Metrika v Kibane&quot;&gt;
&lt;/p&gt;
&lt;p&gt;Išlo napr. o počet modifikovaných uzlov v daný deň (na obrázku vyššie); veľkosť surových dát, ktoré sa stiahli daný deň; sumy záznamov vo výsledných dátach; dĺžku trvania ETL procesu.&lt;br&gt;
Do Kibany reportovali výsledky aj testy, ktoré kontrolovali, či sú prítomné dáta z každej hodiny dňa, či boli všetky surové dáta spracované, či všetky staged dáta boli publikované a či sa na HDFS nenachádzajú staré súbory, ktoré mali byť vymazané po uplynutí doby retencie dát. Nastavila som alerty, aby sme boli o takýchto udalostiach notifikovaní emailom.&lt;/p&gt;
&lt;h3 id=&quot;kvalita-d%C3%A1t%2C-katal%C3%B3g-d%C3%A1t-a-spol.&quot;&gt;Kvalita dát, katalóg dát a spol. &lt;a class=&quot;direct-link&quot; href=&quot;#kvalita-d%C3%A1t%2C-katal%C3%B3g-d%C3%A1t-a-spol.&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Testy boli hotové - vyžadovali len údržbu, monitoring fičal, tak som začala rešeršovať, aké vychytávky by sa ešte dali zaradiť a tým dvihnúť kvalitu našich ETL procesov na ešte vyššiu úroveň. Bohužiaľ, k skúsenostiam s nasledovnými nástrojmi z prvej ruky som sa už nedostala, keďže priority na projekte sa presunuli smerom od big data a neskôr som zmenila pôsobisko.&lt;/p&gt;
&lt;p&gt;Profilovanie dát je proces skúmania zdrojových dát a porozumenia ich štruktúre, obsahu a vzájomných vzťahov s cieľom identifikovania potenciálu pre dátové projekty. Výsledky testovania, monitoringu a profilovania a ich sprievodné vizualizácie môžu pomôcť identifikovať vzory v dátach a príležitosti prepojenia datasetov, značkovania a pridania metadát s cieľom lepšie identifikovať a kategorizovať informácie v organizácii.&lt;/p&gt;
&lt;p&gt;Nástroje na kvalitu dát umožňujú analýzu datasetu, validáciu dát, profilovanie dát, odhalenie nevalidných (napr. duplicitné záznamy, hodnoty v stĺpci nezodpovedajúce dátovému typu alebo rozsahu hodnôt, v akom by mali byť), chýbajúcich alebo neobvyklých dát. Nastavujú sa prahy akceptovaných parametrov pre dáta s dobrou kvalitou, oproti ktorým sa dáta testujú a pravidelne kontrolujú, či spĺňajú vopred definované pravidlá a štandardy kvality. Zámerom bolo zapojiť takéto testy do workflowu a zastaviť ďalšie spracovávanie datasetu, ak by obsahoval zlé dáta a tým zabrániť zavedeniu zlých dát do published.&lt;br&gt;
Pre tieto účely som našla knižnicu &lt;a href=&quot;https://greatexpectations.io/&quot;&gt;Great Expectations&lt;/a&gt;, nemohli sme ju však použiť kvôli nekompatibilnej verzii Pythonu na našom clusteri. Great Expectations umožňuje tiež automatickú dokumentáciu dát. Až neskôr som objavila alternatívy pre Scalu ako &lt;a href=&quot;https://github.com/awslabs/deequ&quot;&gt;Deequ&lt;/a&gt;, &lt;a href=&quot;https://github.com/FRosner/drunken-data-quality&quot;&gt;DDQ&lt;/a&gt;, &lt;a href=&quot;https://github.com/databrickslabs/dataframe-rules-engine&quot;&gt;DataFrame Rules Engine&lt;/a&gt; a &lt;a href=&quot;https://griffin.apache.org/&quot;&gt;Apache Griffin&lt;/a&gt;. &lt;a href=&quot;https://github.com/sodadata/soda-sql&quot;&gt;Soda SQL&lt;/a&gt; operuje nad dátami prístupnými SQL, podporuje Amazon Redshift, Apache Hive a Spark, Snowflake a niekoľko DB a pravidlá sa definujú v YAML formáte. Umožňuje aj používanie vlastných metrík.&lt;/p&gt;
&lt;p&gt;Zaujímavo sa javili aj nástroje na vytvorenie katalógu dát &lt;a href=&quot;https://www.amundsen.io/&quot;&gt;Amundsen&lt;/a&gt; a na pôvod dát a vizualizáciu procesov &lt;a href=&quot;https://absaoss.github.io/spline&quot;&gt;Apache Spline&lt;/a&gt;. Katalóg dát slúži ako organizovaný inventár dát pre dátových inžinierov a používateľov dát, kde môžu na jednom mieste vyhľadávať dáta vhodné pre svoje potreby.&lt;/p&gt;
&lt;p&gt;Pomocou môžu byť aj riešenia založené na strojovom učení (ML), ktoré by sa natrénovali na historických dátach a odhaľovali anomálie v novopríchodzích.&lt;/p&gt;
&lt;h3 id=&quot;z%C3%A1ver&quot;&gt;Záver &lt;a class=&quot;direct-link&quot; href=&quot;#z%C3%A1ver&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;ETL procesy a ich testovanie sú menšinová oblasť IT, ale majú svoje čaro. Postupom času zistíte, kde všade môže nastať problém. Niekoľko takých tipov na čo si dať pozor:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;prehodenie hodnôt medzi dvoma stĺpcami rovnakého dátového typu&lt;/li&gt;
&lt;li&gt;náhle zastavenie dodávania dát od tretej strany&lt;/li&gt;
&lt;li&gt;náhla zmena schémy surových dát&lt;/li&gt;
&lt;li&gt;hodiny dát naviac a menej v dňoch prechodu na letný čas a späť&lt;/li&gt;
&lt;li&gt;uistite sa, že sťahujete naozaj všetky surové dáta z ich zdroja&lt;/li&gt;
&lt;li&gt;uistite sa, že nahrávate naozaj všetky spracované dáta na cieľový systém&lt;/li&gt;
&lt;li&gt;pravidelne zálohujte dáta, môže sa stať, že workflow vymaže/pokazí časť dát&lt;/li&gt;
&lt;li&gt;dobré je aj pravidelne sledovať historické dáta v cieľovom systéme, môže sa stať, že workflow vymaže časť dát, nahrá dáta viackrát alebo prepíše inú partíciu ako má a takto to môžete zistiť&lt;/li&gt;
&lt;li&gt;pozor na upgrady používaných nástrojov. Spark 2.4 zmenil ako sa ukladajú prázdne reťazce z CSV (null/prázdny reťazec), čo malo vplyv na spájanie datasetov na dotknutom stĺpci&lt;/li&gt;
&lt;li&gt;ak beh workflowu prechádza cez polnoc (alebo inú pre Vás relevantnú časovú hranicu), uistite sa, že všetky kroky workflowu pracujú s tými batchmi dát, s ktorými majú&lt;/li&gt;
&lt;/ul&gt;

			
		</content>
	</entry>
</feed>
